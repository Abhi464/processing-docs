<h1 style="line-height: 0.7em;">Sound</h1>
<h3 style="line-height: 0.7em;"><em>Text by R. Luke DuBois, code by Wilm Thoben</em></h3>

<table width="650">
	<tr>
	<td>

    <p class="license">
    	This tutorial is Extension 1 from the second edition of <em><a href="https://mitpress.mit.edu/books/processing-0">Processing: A Programming Handbook for Visual Designers and Artists</a></em>, published by MIT Press. Copyright 2014 MIT Press. This tutorial is for Processing version 2.0+. If you see any errors or have comments, please <a href="https://github.com/processing/processing-docs/issues?state=open">let us know</a>.
    </p>

	<p class="txt">
		The history of music is, in many ways, the history of technology. From developments in the writing and transcription of music (notation) to the design of spaces for the performance of music (acoustics) to the creation of musical instruments, composers and musicians have availed themselves of advances in human understanding to perfect and advance their professions. Unsurprisingly, therefore, we find that in the machine age these same people found themselves first in line to take advantage of the new techniques and possibilities offered by electricity, telecommunications, and, in the last century, digital computers to leverage all of these systems to create new and expressive forms of sonic art. Indeed, the development of phonography (the ability to reproduce sound mechanically) has, by itself, had such a transformative effect on aural culture that it seems inconceivable now to step back to an age where sound could emanate only from its original source.<sup>1</sup> The ability to create, manipulate, and reproduce lossless sound by digital means is having, at the time of this writing, an equally revolutionary effect on how we listen. As a result, the artist today working with sound has not only a huge array of tools to work with, but also a medium exceptionally well suited to technological experimentation.
	</p>
 
	<h3>Music and sound programming in the arts</h3>
	
	<p class="txt">
		Thomas Edison’s 1857 invention of the phonograph and Nikola Tesla’s wireless radio demonstration of 1893 paved the way for what was to be a century of innovation in the electromechanical transmission and reproduction of sound. Emile Berliner’s gramophone record (1887) and the advent of AM radio broadcasting under Guglielmo Marconi (1922) democratized and popularized the consumption of music, initiating a process by which popular music quickly transformed from an art of minstrelsy into a commodified industry worth tens of billions of dollars worldwide.<sup>2</sup> New electronic musical instruments, from the large and impractical telharmonium to the simple and elegant theremin multiplied in tandem with recording and broadcast technologies and prefigured the synthesizers, sequencers, and samplers of today. Many composers of the time were, not unreasonably, entranced by the potential of these new mediums of transcription, transmission, and performance. Luigi Russolo, the futurist composer, wrote in his 1913 manifesto The Art of Noises of a futurist orchestra harnessing the power of mechanical noisemaking (and phonographic reproduction) to “liberate” sound from the tyranny of the merely musical. John Cage, in his 1937 monograph Credo: The Future of Music, wrote this elliptical doctrine:</p>
    	
    	<blockquote>
			The use of noise to make music will continue and increase until we reach a music produced through the aid of electrical instruments which will make available for musical purposes any and all sounds that can be heard. Photoelectric, film, and mechanical mediums for the synthetic production of music will be explored. Whereas, in the past, the point of disagreement has been between dissonance and consonance, it will be, in the immediate future, between noise and so-called musical sounds.<sup>3</sup>
		</blockquote>
		
	<p class="txt">
		The invention and wide adoption of magnetic tape as a medium for the recording of audio signals provided a breakthrough for composers waiting to compose purely with sound. In the early postwar period, the first electronic music studios flourished at radio stations in Paris (ORTF) and Cologne (WDR). The composers at the Paris studio, most notably Pierre Henry and Pierre Schaeffer, developed the early compositional technique of musique concrète, working directly with recordings of sound on phonographs and magnetic tape to construct compositions through a process akin to what we would now recognize as sampling. Schaeffer’s Étude aux chemins de fer (1948) and Henry and Schaeffer’s Symphonie pour un homme seul are classics of the genre. Meanwhile, in Cologne, composers such as Herbert Eimart and Karlheinz Stockhausen were investigating the use of electromechanical oscillators to produce pure sound waves that could be mixed and sequenced with a high degree of precision. This classic elektronische music was closely tied to the serial techniques of the contemporary modernist avant-garde, who were particularly well suited aesthetically to become crucial advocates for the formal quantification and automation offered by electronic and, later, computer music.<sup>4</sup> The Columbia-Princeton Electronic Music Center, founded by Vladimir Ussachevsky, Otto Luening, Milton Babbitt, and Roger Sessions in New York in 1957, staked its reputation on the massive RCA Mark II Sound Synthesizer, a room-sized machine capable of producing and sequencing electronically generated tones with an unprecedented degree of precision and control. In the realm of popular music, pioneering steps were taken in the field of recording engineering, such as the invention of multitrack tape recording by the guitarist Les Paul in 1954. This technology, enabling a single performer to “overdub” her/himself onto multiple individual “tracks” that could later be mixed into a composite, filled a crucial gap in the technology of recording and would empower the incredible boom in recording-studio experimentation that permanently cemented the commercial viability of the studio recording in popular music.
    </p>

    <p class="txt">
		Composers adopted digital computers slowly as a creative tool because of their initial lack of real-time responsiveness and intuitive interface. Although the first documented use of the computer to make music occurred in 1951 on the CSIRAC machine in Sydney, Australia, the genesis of most foundational technology in computer music as we know it today came when Max Mathews, a researcher at Bell Labs in the United States, developed a piece of software for the IBM 704 mainframe called MUSIC. In 1957, the MUSIC program rendered a 17-second composition by Newmann Guttmann called “In the Silver Scale”. Originally tasked with the development of human-comprehensible synthesized speech, Mathews developed a system for encoding and decoding sound waves digitally, as well as a system for designing and implementing digital audio processes computationally. His assumptions about these representational schemes are still largely in use and will be described later in this text. The advent of faster machines, computer music programming languages, and digital systems capable of real-time interactivity brought about a rapid transition from analog to computer technology for the creation and manipulation of sound, a process that by the 1990s was largely comprehensive.5
	</p>

	<p class="txt">
		Sound programmers (composers, sound artists, etc.) use computers for a variety of tasks in the creative process. Many artists use the computer as a tool for the algorithmic and computer-assisted composition of music that is then realized off-line. For Lejaren Hiller’s Illiac Suite for string quartet (1957), the composer ran an algorithm on the computer to generate notated instructions for live musicians to read and perform, much like any other piece of notated music. This computational approach to composition dovetails nicely with the aesthetic trends of twentieth-century musical modernism, including the controversial notion of the composer as “researcher,” best articulated by serialists such as Milton Babbitt and Pierre Boulez, the founder of IRCAM. This use of the computer to manipulate the symbolic language of music has proven indispensable to many artists, some of whom have successfully adopted techniques from computational research in artificial intelligence to attempt the modeling of preexisting musical styles and forms; for example, David Cope’s 5000 works... and Brad Garton’s Rough Raga Riffs use stochastic techniques from information theory such as Markov chains to simulate the music of J. S. Bach and the styles of Indian Carnatic sitar music, respectively. 
	</p>

	<p class="txt">
		If music can be thought of as a set of informatics to describe an organization of sound, the synthesis and manipulation of sound itself is the second category in which artists can exploit the power of computational systems. The use of the computer as a producer of synthesized sound liberates the artist from preconceived notions of instrumental capabilities and allows her/him to focus directly on the timbre of the sonic artifact, leading to the trope that computers allow us to make any sound we can imagine. Composers such as Jean-Claude Risset (The Bell Labs Catalogue), Iannis Xenakis (GENDYN3), and Barry Truax (Riverrun), have seen the computer as a crucial tool in investigating sound itself for compositional possibilities, be they imitative of real instruments (Risset), or formal studies in the stochastic arrangements of synthesized sound masses (Xenakis) using techniques culminating in the principles of granular synthesis (Truax). The computer also offers extensive possibilities for the assembly and manipulation of preexisting sound along the musique concrète model, though with all the alternatives a digital computer can offer. The compositional process of digital sampling, whether used in pop recordings (Brian Eno and David Byrne’s My Life in the Bush of Ghosts, Public Enemy’s Fear of a Black Planet) or conceptual compositions (John Oswald’s Plunderphonics, Chris Bailey’s Ow, My Head), is aided tremendously by the digital form sound can now take. Computers also enable the transcoding of an audio signal into representations that allow for radical reinvestigation, as in the time-stretching works of Leif Inge (9 Beet Stretch, a 24-hour “stretching” of Beethoven’s Ninth Symphony) and the time-lapse phonography of this text’s author (Messiah, a 5-minute “compression” of Handel’s Messiah).
	</p>

	<p class="txt">
		Artists working with sound will often combine the two approaches, allowing for the creation of generative works of sound art where the underlying structural system, as well as the sound generation and delivery, are computationally determined. Artists such as Michael Schumacher, Stephen Vitiello, Carl Stone, and Richard James (the Aphex Twin) all use this approach. Most excitingly, computers offer immense possibilities as actors and interactive agents in sonic performance, allowing performers to integrate algorithmic accompaniment (George Lewis), hyperinstrument design (Laetitia Sonami, Interface), and digital effects processing (Pauline Oliveros, Mari Kimura) into their repertoire.
	</p>

	<p class="txt">
		Now that we’ve talked a bit about the potential for sonic arts on the computer, we’ll investigate some of the specific underlying technologies that enable us to work with sound in the digital domain.
	</p>


	<h3>Sound and musical informatics</h3>
	
	<p class="txt">
		Simply put, we define sound as a vibration traveling through a medium (typically air) that we can perceive through our sense of hearing. Sound propagates as a longitudinal wave that alternately compresses and decompresses the molecules in the matter (e.g., air) through which it travels. As a result, we typically represent sound as a plot of pressure over time:
	</p>

	<img src="imgs/TK.svg" style= "width: 650px; height: 220px" class="tut">

	<p class="txt">
		This time-domain representation of sound provides an accurate portrayal of how sound works in the real world, and, as we shall see shortly, it is the most common representation of sound used in work with digitized audio. When we attempt a technical description of a sound wave, we can easily derive a few metrics to help us better understand what’s going on. In the first instance, by looking at the amount of displacement caused by the sound pressure wave, we can measure the amplitude of the sound. This can be measured on a scientific scale in pascals of pressure, but it is more typically quantified along a logarithmic scale of decibels. If the sound pressure wave repeats in a regular or periodic pattern, we can look at the wavelength of a single iteration of that pattern and from there derive the frequency of that wave. For example, if a sound traveling in a medium at 343 meters per second (the speed of sound in air at room temperature) contains a wave that repeats every half-meter, that sound has a frequency of 686 hertz, or cycles per second. The figure below shows a plot of a cello note sounding at 440 Hz; as a result, the periodic pattern of the waveform (demarcated with vertical lines) repeats every 2.27 ms:
	</p>




	<h3>Notes</h3>

	<p class="txt">
		<ol>
		<li>Douglas Kahn, <em>Noise, Water, Meat: A History of Sound in the Arts</em> (MIT Press, 2001),  p. 10.</li>
		<li>Paul Théberge, <em>Any Sound You Can Imagine: Making Music / Consuming Technology</em> (Wesleyan University Press, 1997),  p. 105.</li>
		<li>John Cage, “Credo: The Future of Music (1937),” in <em>John Cage: An Anthology</em>, edited by Richard Kostelanetz (Praeger, 1970),  p. 52.</li>
		<li>Joel Chadabe, <em>Electric Sound: The Past and Promise of Electronic Music</em> (Prentice Hall, 1996), p. 145.</li>
		<li>Curtis Roads, <em>The Computer Music Tutorial</em> (MIT Press, 1996), p. 43.</li>
		<li>Albert Bregman, <em>Auditory Scene Analysis</em> (MIT Press, 1994),  p. 215.</li>
		</ol>
	</p>



	<h3>Code</h3>

	<hr />

	<p class="txt">Example 1</p>

	<p>
	<pre>
		<span style="color: #666666;">/**</span>
		<span style="color: #666666;">&nbsp;*&nbsp;In&nbsp;this&nbsp;example,&nbsp;five&nbsp;sine&nbsp;waves&nbsp;are&nbsp;layered&nbsp;to&nbsp;construct&nbsp;a&nbsp;cluster&nbsp;</span>
		<span style="color: #666666;">&nbsp;*&nbsp;of&nbsp;frequencies.&nbsp;This&nbsp;method&nbsp;is&nbsp;called&nbsp;additive&nbsp;synthesis.&nbsp;Use&nbsp;the&nbsp;</span>
		<span style="color: #666666;">&nbsp;*&nbsp;mouse&nbsp;position&nbsp;inside&nbsp;the&nbsp;display&nbsp;window&nbsp;to&nbsp;detune&nbsp;the&nbsp;cluster.</span>
		<span style="color: #666666;">&nbsp;*/</span>

		<span style="color: #33997E;">import</span> processing.sound.*;

		SinOsc[]&nbsp;sineWaves;&nbsp;<span style="color: #666666;">// Array of sines</span>
		<span style="color: #E2661A;">float</span>[] sineFreq; <span style="color: #666666;">// Array of frequencies</span>
		<span style="color: #E2661A;">int</span> numSines = 5; <span style="color: #666666;">// Number of oscillators to use</span>

		<span style="color: #33997E;">void</span> <span style="color: #006699;"><b>setup</b></span>() {  
		&nbsp;&nbsp;<span style="color: #006699;">size</span>(640, 360);
		&nbsp;&nbsp;<span style="color: #006699;">background</span>(255);

		&nbsp;&nbsp;sineWaves&nbsp;=&nbsp;<span style="color: #33997E;">new</span> SinOsc[numSines]; <span style="color: #666666;">// Initialize the oscillators</span>
		&nbsp;&nbsp;sineFreq&nbsp;=&nbsp;<span style="color: #33997E;">new</span> <span style="color: #E2661A;">float</span>[numSines]; <span style="color: #666666;">// Initialize array for Frequencies</span>

		&nbsp;&nbsp;<span style="color: #669900;">for</span> (<span style="color: #E2661A;">int</span> i = 0; i &lt; numSines; i++) {
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #666666;">// Calculate the amplitude for each oscillator</span>
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #E2661A;">float</span> sineVolume = (1.0 / numSines) / (i + 1);
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #666666;">// Create the oscillators</span>
		&nbsp;&nbsp;&nbsp;&nbsp;sineWaves[i]&nbsp;=&nbsp;<span style="color: #33997E;">new</span> SinOsc(<span style="color: #33997E;">this</span>);
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #666666;">// Start Oscillators</span>
		&nbsp;&nbsp;&nbsp;&nbsp;sineWaves[i].play();
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #666666;">// Set the amplitudes for all oscillators</span>
		&nbsp;&nbsp;&nbsp;&nbsp;sineWaves[i].amp(sineVolume);
		&nbsp;&nbsp;}
		}

		<span style="color: #33997E;">void</span> <span style="color: #006699;"><b>draw</b></span>() {

		&nbsp;&nbsp;<span style="color: #666666;">//Map mouseY from 0 to 1</span>
		&nbsp;&nbsp;<span style="color: #E2661A;">float</span> yoffset = <span style="color: #006699;">map</span>(<span style="color: #D94A7A;">mouseY</span>, 0, <span style="color: #D94A7A;">height</span>, 0, 1);
		&nbsp;&nbsp;<span style="color: #666666;">//Map mouseY logarithmically to 150 - 1150 to create a base frequency range</span>
		&nbsp;&nbsp;<span style="color: #E2661A;">float</span> frequency = <span style="color: #006699;">pow</span>(1000, yoffset) + 150;
		&nbsp;&nbsp;<span style="color: #666666;">//Use mouseX mapped from -0.5 to 0.5 as a detune argument</span>
		&nbsp;&nbsp;<span style="color: #E2661A;">float</span> detune = <span style="color: #006699;">map</span>(<span style="color: #D94A7A;">mouseX</span>, 0, <span style="color: #D94A7A;">width</span>, -0.5, 0.5);

		&nbsp;&nbsp;<span style="color: #669900;">for</span> (<span style="color: #E2661A;">int</span> i = 0; i &lt; numSines; i++) { 
		&nbsp;&nbsp;&nbsp;&nbsp;sineFreq[i]&nbsp;=&nbsp;frequency&nbsp;*&nbsp;(i&nbsp;+&nbsp;1&nbsp;*&nbsp;detune);
		&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #666666;">// Set the frequencies for all oscillators</span>
		&nbsp;&nbsp;&nbsp;&nbsp;sineWaves[i].freq(sineFreq[i]);
		&nbsp;&nbsp;}
		}
	</pre>
	</p>

	<hr />

	<p class="txt">Example 2</p>



	</td></tr>
 </table>